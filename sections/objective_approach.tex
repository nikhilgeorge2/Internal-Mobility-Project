\section{Objective and Approach}\label{sec:objective_approach}

Our central objective is to measure how informative job postings are about position requirements. This presents a 
fundamental measurement challenge: how can we quantify the information content in posting text about actual 
position requirements? We approach this challenge by examining selection patterns in internal labor markets, 
which can reveal the information about position requirements and skill demands inherent within job posting text.

Job postings attempt to communicate complex technical requirements through structured but varied language. 
Similar skills might be described differently across postings, while seemingly different descriptions could 
indicate similar requirements. Internal labor markets provide an ideal setting for measuring posting 
informativeness, as we observe both detailed position descriptions and the outcomes of workers moving between 
positions. If postings contain meaningful information about requirements, then differences in posting content 
should help predict these movement patterns.

To precisely define our measurement approach, let us consider the evaluation of a single internal applicant 
for a position. Let $j_c$ denote the job description of the internal applicant's current position and $j_v$ 
the job description of the vacancy. During selection, the decision maker observes features $z_i$ of the 
applicant that we cannot observe (such as interview performance, specific project experience, or demonstrated 
soft skills), assesses a distance $\delta(j_v, (j_c, z_i))$, and selects the applicant if:

[
S(j_v, (j_c, z_i)) =
\begin{cases}
1, & \text{if } \delta(j_v, (j_c, z_i)) < \tau \
0, & \text{if } \delta(j_v, (j_c, z_i)) \geq \tau
\end{cases}
]

This selection rule indicates that if the distance between the vacancy and the applicant's profile 
(based on their current role and unobserved features) is below a threshold, they are selected. Informative 
job postings would lead to a distance metric where this threshold effectively separates qualified and 
unqualified candidates based on the information contained in posting text alone. We can quantify this 
informativeness using the Area Under the Receiver Operating Characteristic Curve (AUC), which measures 
how well our learned distances distinguish between successful and unsuccessful transitions.

We observe a set $A = {(j_v, j_c, S_{v,c})}$ of past decisions, where each observation contains the vacancy 
posting, the applicant's current position posting, and the selection outcome. Our measurement approach 
focuses on finding a representation function $f$ that maps job descriptions ${j_i}$ to a set of 
vectors ${x_i}$, where $x_i = f(j_i)$. Each vector $x_i$ encodes information about the skills and 
responsibilities described in $j_i$. Our goal is then to learn an appropriate distance metric $d(x_v, x_c)$ 
defined on the set ${x_i}$ such that $d(x_v, x_c)$ serves as a stand-in for $\delta(j_v, (j_c, z_i))$.


We employ two key tools to measure posting informativeness: pre-trained language models that provide 
systematic representations of posting content, and metric learning that reveals which differences in 
these representations matter for position transitions. These tools serve to measure, not create, 
information - like calibrating an instrument to detect existing signals. While we leverage selection 
patterns to guide the metric learning process, this does not inject new information into the job postings. 
Instead, it allows us to discover and quantify the information already present in the posting content that 
aligns with actual hiring outcomes.

[DIAGRAM PLACEHOLDER: Conceptual overview showing flow from job postings through representation and metric learning to measurement of informativeness]

A key implementation challenge arises from the high dimensionality of language model representations. 
In high dimensions, most points tend to be roughly the same distance apart; as distance measure is 
important to us, we want to bring down the dimensionality of our representations. Moreover, while 
these representations are rich, our selection data for supervised metric learning is limited. The 
skills and requirements relevant to IT positions likely occupy a lower-dimensional manifold within 
the embedding space, suggesting that dimension reduction can preserve the relevant information while 
enabling effective measurement through metric learning.

The following section details our complete implementation pipeline. First, we leverage the language capabilities 
of pre-trained transformer models, which excel at capturing subtle differences in technical skill requirements. 
Second, we utilize selection patterns in a supervised capacity to learn which aspects of posting content are most 
informative. By learning a distance metric where differences in posting content predict selection patterns, we 
can measure how informative these postings are about actual position requirements. This measurement reveals 
whether postings contain meaningful information about requirements or merely serve as generic templates.