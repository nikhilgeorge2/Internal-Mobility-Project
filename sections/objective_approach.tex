
\section{Objective and Approach}\label{sec:objective_approach}

To test whether job postings contain meaningful information about skill requirements, we develop a distance measure between positions based on their posting content. If postings are informative about skills and requirements, this distance should be inversely related to the probability of selection when a person working in one position applies to fill another. In this section, we outline our goal and approach to learning the distance metric and introduce notation before discussing the details in the following section. To build intuition, let us consider the evaluation of a single internal applicant for a position. Let \(j_c\) denote the job description of the internal applicant and \(j_v\) the job description of the vacancy. There is some \(z_i\), a feature vector of the applicant observed in the selection process, which we do not observe. The decision maker assesses a distance \(\delta(j_v, (j_c, z_i))\) and selects the applicant if:
\[
S(j_v, (j_c, z_i)) = 
\begin{cases} 
1, & \text{if } \delta(j_v, (j_c, z_i)) < \tau \\
0, & \text{if } \delta(j_v, (j_c, z_i)) \geq \tau
\end{cases}
\]
Here, \(S\) is a binary selection indicator. We observe the set \(A = \{(j_v, j_c, S)\}\) of past decisions, where \(S\) is the observed selection decision. Although we do not observe the feature vector \(z_i\) available to the decision maker, we use the set \(A\) to verify that the probability of selection \(\text{Pr}(S = 1 \mid x_v, x_c)\) is decreasing in \(d(x_v, x_c)\). Our goal is to find a representation function \(f\) that maps job descriptions \(\{j_i\}\) to a set of vectors \(\{x_i\}\), where \(x_i = f(j_i)\). Each vector \(x_i\) encodes information about the skills and responsibilities described in \(j_i\). We then want to choose an appropriate distance metric \(d(x_v, x_c)\) defined on the set \(\{x_i\}\) such that \(d(x_v, x_c)\) serves as a stand-in for \(\delta(j_v, (j_c, z_i))\). We aim to choose \(f\) and \(d\) so that a classifier predicting whether an applicant is selected for the position they applied for, based on \(d(x_v, x_c)\), has a high Area Under the Receiver Operating Characteristic Curve (AUC).


Success in predicting selection through the skill distance measure would validate that job postings capture meaningful differences in position requirements. Rather than relying on a uniform probability estimate of selection, the ability to generate varying probabilities based on posting content would demonstrate that these descriptions contain valuable and discriminative information about skill fit. If posting content is informative, the distance between job profile vectors should enable us to distinguish between more and less likely selections based purely on the documented position requirements. This would confirm that the detailed skill descriptions in postings meaningfully capture actual job requirements rather than serving as generic templates.


We will now briefly discuss how we approach testing posting informativeness through this prediction task, before presenting a more detailed version in the following section. Our approach involves mapping job descriptions \(\{j_i\}\) to vectors \(\{x_i\}\) so that the distance \(d(x_v, x_c)\) inversely correlates with the probability of selection when a worker in current job \(c\) applies to vacancy \(v\). Two key ideas guide our approach to choosing the representation and distance metric. First, we leverage the language capabilities of a pre-trained transformer-based language model. While the specialized vocabulary used in IT job descriptions could be represented using more classical NLP approaches, the advanced language capabilities of a transformer model offer significant advantages in capturing subtle differences in skill requirements. Second, we utilize our selection and rejection data in a supervised capacity during the selection process for the representation and distance metric. This means we don't merely use the set of internal applications whose outcomes we observe for validation; we also incorporate this data to learn which aspects of posting content are most informative for predicting selection.





\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[
        node distance=2cm,
        box/.style={rectangle, draw, rounded corners, minimum width=3cm, minimum height=1.5cm},
        post/.style={rectangle, draw, fill=gray!10, minimum width=2.5cm, minimum height=2cm},
        data/.style={rectangle, draw, dashed, fill=gray!5, minimum width=2.5cm},
        flow/.style={->, thick},
        guide/.style={->, thick, dashed}
    ]
    
    % Left side: Job Postings
    \node[post] (post1) at (0,2) {Job Posting A\\Skills: Python\\Requirements: ML};
    \node[post] (post2) at (0,-2) {Job Posting B\\Skills: Java\\Requirements: Cloud};
    
    % Pre-trained Model Layer
    \node[box] (model) at (4,0) {Pre-trained Language Model};
    
    % Vector Representations
    \node[box] (vec1) at (8,2) {Vector A};
    \node[box] (vec2) at (8,-2) {Vector B};
    
    % Metric Learning Component
    \node[box] (metric) at (12,0) {Metric Learning};
    
    % Selection Patterns
    \node[data] (select) at (12,-4) {Selection Patterns\\(Measurement Guide)};
    
    % Output Distance Metric
    \node[box] (distance) at (16,0) {Learned\\Distance Metric};
    
    % Information Flow Arrows
    \draw[flow] (post1) -- node[above] {text} (model);
    \draw[flow] (post2) -- node[below] {text} (model);
    \draw[flow] (model) -- node[above] {embeddings} (vec1);
    \draw[flow] (model) -- node[below] {embeddings} (vec2);
    \draw[flow] (vec1) -- node[above] {} (metric);
    \draw[flow] (vec2) -- node[below] {} (metric);
    \draw[flow] (metric) -- node[above] {d(x_v, x_c)} (distance);
    
    % Guidance Flow
    \draw[guide] (select) -- node[right] {guides learning} (metric);
    
    % Labels for main components
    \node[text width=3cm, align=center] at (0,3.5) {Information Source};
    \node[text width=3cm, align=center] at (4,3.5) {Processing};
    \node[text width=3cm, align=center] at (12,3.5) {Measurement};
    \node[text width=3cm, align=center] at (16,3.5) {Output};
    
    \end{tikzpicture}
    \caption{Overview of our measurement approach. Job posting content flows through pre-trained language models to create vector representations. Selection patterns guide the metric learning process to reveal which differences in these representations matter, resulting in a learned distance metric that measures posting informativeness.}
    \label{fig:approach}
\end{figure}

\autoref{fig:ml_pipeline} pictorially outlines our broad approach. The first major idea is to leverage the capabilities of a pre-trained language model. We begin with standard pre-processing of job posting texts, then utilize pre-trained embeddings. In our context, embeddings are numerical representations of entire sentences or paragraphs from job descriptions. These embeddings capture the complex language patterns and specialized vocabulary often found in IT job descriptions. This aligns with our use of a pre-trained transformer-based language model, which excels at generating such representations. This approach provides a rich initial representation of the required skills and responsibilities, setting the foundation for our distance metric learning process. Using embeddings allows us to quantify and compare job descriptions more effectively than traditional text-based methods.

The rich and dense embeddings from general-purpose language models are representations in a much larger space. Our goal is to measure distances between these objects, which lie in a smaller subspace. In high dimensions, most points tend to be roughly the same distance apart; as distance measure is important to us, we want to bring down the dimensionality of our representations. A second motivation is our interest in using the selection/rejection data in a supervising framework to learn a distance metric. The core idea is to learn from data a matrix $M$ to re-weight the distance between the respective representations of two job descriptions. As the observations of internal applications are limited, our ability to reweigh the distance measure is affected with high dimensions, in addition to the aforementioned curse of dimensionality problem of distances becoming less meaningful at higher dimensions. We apply dimension reduction techniques to transition the high-dimensional embeddings to more manageable vectors, facilitating more effective distance metric learning while retaining the most relevant information.


The third aspect of our data analytic framework involves using a distance metric that is fine-tuned to our historical selection and rejection data in a supervised capacity. This ensures that the learned metric is tailored to our specific context. The key idea is that differences along features that matter less for selection should weigh less than differences along features that matter more. Metric learning is a well-established idea in machine learning, which we can apply to learn a custom distance between any pair of job representations. This approach allows us to incorporate the valuable information contained in our past hiring decisions directly into our distance measure.

In practice, we must choose the dimensions of our representations carefully. Too few dimensions would not capture the differences between job profiles adequately, while too many would dampen our ability to take advantage of the supervised framework. We can select the optimal dimensionality of the representation via cross-validation. To measure effectiveness, we will utilize the Area Under the Curve (AUC), allowing us to iteratively adjust our approach. This process of validation and adjustment ensures that our distance metric continually improves in its ability to capture the selection patterns within the firm.


This pipeline integrates pre-trained language models, dimensionality reduction, and supervised distance metric learning, making use of historical application and selection data. By actively crafting our representation and distance metric using real selection data, we can test whether job posting content meaningfully captures differences in skill requirements beyond simple text similarity. With this approach, we aim to validate whether postings contain sufficient information to predict selection outcomes, setting the foundation for examining how informative these descriptions are about actual position requirements and worker-job fit.
